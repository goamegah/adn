#!/usr/bin/env python3
"""
Script d'import MIMIC-III vers Cloud SQL
Version simplifi√©e pour votre cas d'usage
"""

import os
import sys
import argparse
import re
from pathlib import Path
from urllib.parse import quote_plus  # ‚úÖ AJOUT IMPORTANT
import pandas as pd
from sqlalchemy import create_engine
from google.cloud import storage, secretmanager
import logging

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Tables MIMIC-III
TABLES = [
    "ADMISSIONS", "CALLOUT", "CAREGIVERS", "CHARTEVENTS", "CPTEVENTS",
    "DATETIMEEVENTS", "D_CPT", "DIAGNOSES_ICD", "D_ICD_DIAGNOSES",
    "D_ICD_PROCEDURES", "D_ITEMS", "D_LABITEMS", "DRGCODES", "ICUSTAYS",
    "INPUTEVENTS_CV", "INPUTEVENTS_MV", "LABEVENTS", "MICROBIOLOGYEVENTS",
    "NOTEEVENTS", "OUTPUTEVENTS", "PATIENTS", "PRESCRIPTIONS",
    "PROCEDUREEVENTS_MV", "PROCEDURES_ICD", "SERVICES", "TRANSFERS"
]

def clean_db_host(db_host: str) -> str:
    """Nettoie et valide l'IP/hostname de la base de donn√©es"""
    if not db_host:
        raise ValueError("‚ùå DB_HOST est vide")
    
    # Extraire seulement l'IP si elle contient des caract√®res parasites
    # Chercher un pattern IP valide (xxx.xxx.xxx.xxx)
    ip_pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}\b'
    match = re.search(ip_pattern, db_host)
    
    if match:
        clean_ip = match.group(0)
        if clean_ip != db_host:
            logger.warning(f"‚ö†Ô∏è  DB_HOST nettoy√©: '{db_host}' ‚Üí '{clean_ip}'")
        return clean_ip
    
    # Si pas de pattern IP trouv√©, retourner tel quel (peut √™tre un hostname)
    return db_host

def get_secret(project_id: str, secret_id: str) -> str:
    """R√©cup√®re un secret depuis Secret Manager"""
    try:
        client = secretmanager.SecretManagerServiceClient()
        name = f"projects/{project_id}/secrets/{secret_id}/versions/latest"
        response = client.access_secret_version(request={"name": name})
        return response.payload.data.decode("UTF-8")
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la r√©cup√©ration du secret {secret_id}: {e}")
        raise

def create_db_engine(env: str, project_id: str):
    """Cr√©e le moteur SQLAlchemy"""
    db_user = os.getenv("DB_USER", "adn_user")
    db_name = os.getenv("DB_NAME", "adn_database")
    db_host_raw = os.getenv("DB_HOST")
    db_port = os.getenv("DB_PORT", "5432")
    
    if not db_host_raw:
        raise ValueError("‚ùå DB_HOST n'est pas d√©fini dans les variables d'environnement")
    
    # Nettoyer l'IP
    db_host = clean_db_host(db_host_raw)
    
    logger.info(f"üîó Connexion √† la base de donn√©es:")
    logger.info(f"  ‚Ä¢ Host (raw): {db_host_raw}")
    logger.info(f"  ‚Ä¢ Host (clean): {db_host}")
    logger.info(f"  ‚Ä¢ Port: {db_port}")
    logger.info(f"  ‚Ä¢ Database: {db_name}")
    logger.info(f"  ‚Ä¢ User: {db_user}")
    
    # R√©cup√©rer le password depuis Secret Manager
    secret_id = f"adn-app-db-password-{env}"
    logger.info(f"üîë R√©cup√©ration du mot de passe depuis Secret Manager ({secret_id})...")
    db_password = get_secret(project_id, secret_id)
    
    # ‚úÖ ENCODER le mot de passe pour g√©rer les caract√®res sp√©ciaux (@, :, /, etc.)
    db_password_encoded = quote_plus(db_password)
    logger.info(f"üîí Mot de passe encod√© pour URL (longueur: {len(db_password_encoded)} caract√®res)")
    
    connection_string = (
        f"postgresql://{db_user}:{db_password_encoded}@{db_host}:{db_port}/{db_name}"
    )
    
    logger.info("‚úÖ Cr√©ation du moteur SQLAlchemy...")
    return create_engine(
        connection_string,
        connect_args={"connect_timeout": 30},
        pool_pre_ping=True,
        echo=False
    )

def download_from_gcs(bucket_name: str, blob_name: str, local_path: Path):
    """T√©l√©charge un fichier depuis GCS"""
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        
        if not blob.exists():
            raise FileNotFoundError(f"‚ùå Le fichier {blob_name} n'existe pas dans le bucket {bucket_name}")
        
        blob.download_to_filename(str(local_path))
        file_size_mb = local_path.stat().st_size / (1024 * 1024)
        logger.info(f"‚úÖ T√©l√©charg√©: {blob_name} ({file_size_mb:.2f} MB)")
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du t√©l√©chargement de {blob_name}: {e}")
        raise

def import_csv_table(engine, csv_path: Path, table_name: str):
    """Import un CSV dans PostgreSQL"""
    try:
        logger.info(f"üì• Import de {table_name}...")
        
        # Lire le CSV
        df = pd.read_csv(csv_path, low_memory=False)
        logger.info(f"  ‚Ä¢ {len(df):,} lignes d√©tect√©es")
        logger.info(f"  ‚Ä¢ {len(df.columns)} colonnes: {', '.join(df.columns[:5].tolist())}{'...' if len(df.columns) > 5 else ''}")
        
        # Normaliser les noms de colonnes
        df.columns = df.columns.str.lower().str.replace(' ', '_')
        
        # Import par chunks
        logger.info(f"  ‚Ä¢ Import en cours...")
        df.to_sql(
            table_name.lower(),
            engine,
            if_exists="replace",
            index=False,
            method="multi",
            chunksize=5000
        )
        
        logger.info(f"‚úÖ {table_name}: {len(df):,} lignes import√©es")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur sur {table_name}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def main():
    parser = argparse.ArgumentParser(description="Import MIMIC-III vers Cloud SQL")
    parser.add_argument("--env", required=True, choices=["staging", "prod"])
    parser.add_argument("--project-id", required=True)
    parser.add_argument("--bucket", required=True)
    parser.add_argument("--subset", type=int, help="Nombre de tables √† importer (test)")
    
    args = parser.parse_args()
    
    logger.info("=" * 80)
    logger.info(f"üöÄ Import MIMIC-III vers Cloud SQL ({args.env})")
    logger.info(f"üì¶ Project ID: {args.project_id}")
    logger.info(f"üóÑÔ∏è  Bucket: {args.bucket}")
    logger.info("=" * 80)
    
    # Afficher les variables d'environnement pour debug
    logger.info("üîç Variables d'environnement:")
    logger.info(f"  ‚Ä¢ DB_HOST (env): {os.getenv('DB_HOST', 'NOT SET')}")
    logger.info(f"  ‚Ä¢ DB_PORT (env): {os.getenv('DB_PORT', 'NOT SET')}")
    logger.info(f"  ‚Ä¢ DB_NAME (env): {os.getenv('DB_NAME', 'NOT SET')}")
    logger.info(f"  ‚Ä¢ DB_USER (env): {os.getenv('DB_USER', 'NOT SET')}")
    logger.info("")
    
    # Cr√©er le moteur DB
    try:
        engine = create_db_engine(args.env, args.project_id)
        # Test de connexion
        with engine.connect() as conn:
            logger.info("‚úÖ Connexion √† la base de donn√©es r√©ussie")
    except Exception as e:
        logger.error(f"‚ùå Impossible de se connecter √† la base de donn√©es: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 1
    
    # Dossier temporaire
    temp_dir = Path("/tmp/mimic_import")
    temp_dir.mkdir(exist_ok=True)
    logger.info(f"üìÅ Dossier temporaire: {temp_dir}")
    
    # D√©terminer les tables √† importer
    tables_to_import = TABLES[:args.subset] if args.subset else TABLES
    logger.info(f"üìä Nombre de tables √† importer: {len(tables_to_import)}")
    logger.info("")
    
    # Import de chaque table
    success_count = 0
    failed_tables = []
    
    for i, table in enumerate(tables_to_import, 1):
        logger.info(f"{'=' * 80}")
        logger.info(f"üìä Table {i}/{len(tables_to_import)}: {table}")
        logger.info(f"{'=' * 80}")
        
        csv_filename = f"{table}.csv"
        local_path = temp_dir / csv_filename
        
        try:
            # T√©l√©charger depuis GCS
            download_from_gcs(args.bucket, csv_filename, local_path)
            
            # Importer dans Cloud SQL
            if import_csv_table(engine, local_path, table):
                success_count += 1
            else:
                failed_tables.append(table)
            
            # Nettoyer
            if local_path.exists():
                local_path.unlink()
                logger.info(f"üóëÔ∏è  Fichier temporaire supprim√©")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur globale sur {table}: {e}")
            import traceback
            logger.error(traceback.format_exc())
            failed_tables.append(table)
            
            # Nettoyer en cas d'erreur
            if local_path.exists():
                local_path.unlink()
        
        logger.info("")
    
    # R√©sum√©
    logger.info("=" * 80)
    logger.info("üìä R√âSUM√â DE L'IMPORT")
    logger.info("=" * 80)
    logger.info(f"‚úÖ Tables import√©es avec succ√®s: {success_count}/{len(tables_to_import)}")
    
    if failed_tables:
        logger.warning(f"‚ö†Ô∏è  Tables √©chou√©es ({len(failed_tables)}): {', '.join(failed_tables)}")
        logger.info("=" * 80)
        return 1
    else:
        logger.info("üéâ Toutes les tables ont √©t√© import√©es avec succ√®s !")
        logger.info("=" * 80)
        return 0

if __name__ == "__main__":
    sys.exit(main())