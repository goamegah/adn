#!/usr/bin/env python3
"""
Scraper simple avec Selenium pour le Guide de R√©gulation M√©dicale
Clique sur chaque fiche et r√©cup√®re le texte visible
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time
import os
import re
from datetime import datetime

class SimpleSeleniumScraper:
    def __init__(self, output_dir="guide_medical_textes"):
        """
        Initialise le scraper avec Selenium
        
        Args:
            output_dir: Dossier o√π sauvegarder les textes
        """
        self.output_dir = output_dir
        self.base_url = "https://www.guide-regulation-medicale.fr/index.php?module=Fiche&action=ListView"
        
        print("üöÄ D√©marrage du scraper...")
        print(f"üìÅ Les fichiers seront sauvegard√©s dans: {output_dir}/")
        
        # Cr√©er le dossier principal
        os.makedirs(output_dir, exist_ok=True)
        
        # Configuration du navigateur
        self.setup_driver()
        
        # Compteurs
        self.total_fiches = 0
        self.fiches_scrapees = 0
        self.erreurs = 0
    
    def setup_driver(self):
        """Configure le navigateur Chrome/Firefox"""
        try:
            # Essayer Chrome d'abord
            options = webdriver.ChromeOptions()
            options.add_argument('--headless')  # Mode sans fen√™tre
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--window-size=1920,1080')
            options.add_experimental_option('excludeSwitches', ['enable-logging'])
            
            self.driver = webdriver.Chrome(options=options)
            print("‚úÖ Chrome configur√© en mode headless")
        except:
            try:
                # Si Chrome ne marche pas, essayer Firefox
                options = webdriver.FirefoxOptions()
                options.add_argument('--headless')
                self.driver = webdriver.Firefox(options=options)
                print("‚úÖ Firefox configur√© en mode headless")
            except Exception as e:
                print("‚ùå Erreur: Impossible de lancer le navigateur")
                print("Installez Chrome ou Firefox et leurs drivers:")
                print("  pip install selenium")
                print("  Chrome: t√©l√©charger chromedriver")
                print("  Firefox: t√©l√©charger geckodriver")
                raise e
    
    def nettoyer_nom_fichier(self, texte):
        """Nettoie un texte pour en faire un nom de fichier valide"""
        # Enlever les caract√®res sp√©ciaux
        texte = re.sub(r'[<>:"/\\|?*]', '', texte)
        # Remplacer les espaces multiples
        texte = re.sub(r'\s+', ' ', texte)
        # Limiter la longueur
        texte = texte[:100]
        return texte.strip()
    
    def creer_dossier_categorie(self, titre):
        """Cr√©e un sous-dossier bas√© sur la cat√©gorie du titre"""
        # D√©terminer la cat√©gorie bas√©e sur des mots-cl√©s
        categories = {
            'cardio': ['cardiaque', 'c≈ìur', 'palpitation', 'hypertens', 'thoracique'],
            'respiratoire': ['respiratoire', 'dyspn√©e', 'asthme', 'thorax'],
            'neurologie': ['neurolog', 'convulsion', 'c√©phal√©e', 'malaise', 'AVC'],
            'pediatrie': ['enfant', 'nourrisson', 'p√©diatr', 'nouveau-n√©'],
            'traumato': ['trauma', 'chute', 'plaie', 'fracture', 'accident'],
            'psychiatrie': ['psychiatr', 'suicid', 'angoisse', 'agitation'],
            'obstetrique': ['grossesse', 'accouchement', 'obst√©tric', 'enceinte'],
            'intoxication': ['intoxic', 'poison', 'overdose', 'drogue'],
            'urgences': ['urgence', 'SMUR', 'SAMU', 'r√©gulation'],
            'divers': []  # Cat√©gorie par d√©faut
        }
        
        titre_lower = titre.lower()
        
        for categorie, mots_cles in categories.items():
            if categorie == 'divers':
                continue
            for mot in mots_cles:
                if mot in titre_lower:
                    path = os.path.join(self.output_dir, categorie)
                    os.makedirs(path, exist_ok=True)
                    return categorie
        
        # Si aucune cat√©gorie trouv√©e, mettre dans divers
        path = os.path.join(self.output_dir, 'divers')
        os.makedirs(path, exist_ok=True)
        return 'divers'
    
    def scraper_toutes_les_fiches(self):
        """Parcourt et scrape toutes les fiches du site"""
        print(f"\nüìã Chargement de la page principale...")
        print(f"URL: {self.base_url}")
        
        # Aller sur la page de liste
        self.driver.get(self.base_url)
        time.sleep(3)  # Attendre le chargement
        
        # Trouver tous les liens des fiches
        print("\nüîç Recherche des fiches...")
        try:
            # Attendre que la page se charge
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.TAG_NAME, "a"))
            )
            
            # R√©cup√©rer tous les liens
            liens = self.driver.find_elements(By.XPATH, "//a[contains(@href, 'FrontDetailView')]")
            
            # Stocker les infos des fiches
            fiches = []
            for lien in liens:
                titre = lien.text.strip()
                url = lien.get_attribute('href')
                if titre and url:
                    fiches.append({
                        'titre': titre,
                        'url': url
                    })
            
            self.total_fiches = len(fiches)
            print(f"‚úÖ {self.total_fiches} fiches trouv√©es!")
            
            # Cr√©er un fichier index
            index_path = os.path.join(self.output_dir, '_INDEX.txt')
            with open(index_path, 'w', encoding='utf-8') as f:
                f.write(f"INDEX DES FICHES - Guide de R√©gulation M√©dicale\n")
                f.write(f"G√©n√©r√© le: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Total: {self.total_fiches} fiches\n")
                f.write("=" * 80 + "\n\n")
                
                for i, fiche in enumerate(fiches, 1):
                    f.write(f"{i:3d}. {fiche['titre']}\n")
            
            # Scraper chaque fiche
            for i, fiche in enumerate(fiches, 1):
                self.scraper_une_fiche(fiche, i)
                
        except TimeoutException:
            print("‚ùå Timeout: La page met trop de temps √† charger")
        except Exception as e:
            print(f"‚ùå Erreur lors de la r√©cup√©ration des liens: {e}")
    
    def scraper_une_fiche(self, fiche, numero):
        """Scrape une fiche individuelle"""
        titre = fiche['titre']
        url = fiche['url']
        
        print(f"\nüìÑ [{numero}/{self.total_fiches}] {titre[:50]}...")
        
        try:
            # Aller sur la page de la fiche
            self.driver.get(url)
            time.sleep(2)  # Attendre le chargement
            
            # Attendre que le contenu se charge
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # R√©cup√©rer tout le texte visible de la page
            body = self.driver.find_element(By.TAG_NAME, "body")
            texte_complet = body.text
            
            if not texte_complet or len(texte_complet) < 50:
                print(f"   ‚ö†Ô∏è Peu ou pas de contenu trouv√©")
                self.erreurs += 1
                return
            
            # D√©terminer la cat√©gorie et cr√©er le dossier
            categorie = self.creer_dossier_categorie(titre)
            
            # Cr√©er le nom du fichier
            nom_fichier = f"{numero:03d}_{self.nettoyer_nom_fichier(titre)}.txt"
            chemin_fichier = os.path.join(self.output_dir, categorie, nom_fichier)
            
            # Sauvegarder le texte
            with open(chemin_fichier, 'w', encoding='utf-8') as f:
                # En-t√™te du fichier
                f.write("=" * 80 + "\n")
                f.write(f"FICHE N¬∞{numero}\n")
                f.write("=" * 80 + "\n\n")
                f.write(f"TITRE: {titre}\n")
                f.write(f"URL: {url}\n")
                f.write(f"DATE DE R√âCUP√âRATION: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"CAT√âGORIE: {categorie.upper()}\n")
                f.write("\n" + "=" * 80 + "\n")
                f.write("CONTENU:\n")
                f.write("=" * 80 + "\n\n")
                
                # Le contenu
                f.write(texte_complet)
                
                # Pied de page
                f.write("\n\n" + "=" * 80 + "\n")
                f.write("FIN DU DOCUMENT\n")
                f.write("=" * 80 + "\n")
            
            self.fiches_scrapees += 1
            print(f"   ‚úÖ Sauvegard√© dans: {categorie}/{nom_fichier}")
            print(f"   üìä {len(texte_complet)} caract√®res r√©cup√©r√©s")
            
        except TimeoutException:
            print(f"   ‚ùå Timeout sur cette fiche")
            self.erreurs += 1
        except Exception as e:
            print(f"   ‚ùå Erreur: {e}")
            self.erreurs += 1
    
    def generer_rapport(self):
        """G√©n√®re un rapport final du scraping"""
        rapport_path = os.path.join(self.output_dir, '_RAPPORT.txt')
        
        with open(rapport_path, 'w', encoding='utf-8') as f:
            f.write("RAPPORT DE SCRAPING\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Site: Guide de R√©gulation M√©dicale\n\n")
            f.write("STATISTIQUES:\n")
            f.write("-" * 40 + "\n")
            f.write(f"Fiches trouv√©es: {self.total_fiches}\n")
            f.write(f"Fiches r√©cup√©r√©es avec succ√®s: {self.fiches_scrapees}\n")
            f.write(f"Erreurs: {self.erreurs}\n")
            f.write(f"Taux de r√©ussite: {(self.fiches_scrapees/max(self.total_fiches,1))*100:.1f}%\n\n")
            
            # Lister les cat√©gories et leur contenu
            f.write("ORGANISATION DES FICHIERS:\n")
            f.write("-" * 40 + "\n")
            for categorie in os.listdir(self.output_dir):
                if categorie.startswith('_'):
                    continue
                path = os.path.join(self.output_dir, categorie)
                if os.path.isdir(path):
                    nb_fichiers = len([f for f in os.listdir(path) if f.endswith('.txt')])
                    f.write(f"  {categorie}/: {nb_fichiers} fiches\n")
        
        print(f"\nüìä Rapport sauvegard√©: {rapport_path}")
    
    def run(self):
        """Lance le scraping complet"""
        try:
            print("\n" + "=" * 80)
            print("D√âBUT DU SCRAPING")
            print("=" * 80)
            
            # Scraper toutes les fiches
            self.scraper_toutes_les_fiches()
            
            # G√©n√©rer le rapport
            self.generer_rapport()
            
            print("\n" + "=" * 80)
            print("SCRAPING TERMIN√â!")
            print("=" * 80)
            print(f"\n‚úÖ Succ√®s: {self.fiches_scrapees}/{self.total_fiches} fiches")
            print(f"‚ùå Erreurs: {self.erreurs}")
            print(f"üìÅ Fichiers sauv√©s dans: {self.output_dir}/")
            
        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è Scraping interrompu par l'utilisateur")
        except Exception as e:
            print(f"\n‚ùå Erreur fatale: {e}")
        finally:
            # Fermer le navigateur
            if hasattr(self, 'driver'):
                self.driver.quit()
                print("\nüîí Navigateur ferm√©")


def main():
    """Fonction principale"""
    import sys
    
    # V√©rifier si un dossier de sortie est sp√©cifi√©
    output_dir = sys.argv[1] if len(sys.argv) > 1 else "guide_medical_textes"
    
    print("üè• SCRAPER - Guide de R√©gulation M√©dicale")
    print("=" * 80)
    print("Ce script va:")
    print("  1. Ouvrir un navigateur en mode invisible")
    print("  2. Aller sur chaque fiche m√©dicale")
    print("  3. R√©cup√©rer tout le texte visible")
    print("  4. Organiser les fichiers par cat√©gorie")
    print("=" * 80)
    
    # Demander confirmation
    reponse = input("\n‚ñ∂Ô∏è  Appuyez sur Entr√©e pour commencer (ou Ctrl+C pour annuler)...")
    
    # Lancer le scraper
    scraper = SimpleSeleniumScraper(output_dir=output_dir)
    scraper.run()


if __name__ == "__main__":
    main()